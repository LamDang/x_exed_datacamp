{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datacamp: Claims scoring for automatic acceptation for BNP Paribas Cardif"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The dataset is stored in `DataFrame_Claims_Spain.csv` file.\n",
    "\n",
    "This datacamp is dedicated to the development of a machine learning pipeline to automatize the claim process. In short, automatic acceptation will speed-up the procedure, improve customer experience, and reduce management costs. However, it will be counterbalanced by an increase in the number of payments due to false positive acceptations leading to some new costs. Therefore, the pipeline should outcome probabilities to find the appropriate trade-off between accepting claims and creating new costs.\n",
    "\n",
    "## 2. Available features\n",
    "\n",
    "The handed data set contains the following features which can be used for designing the machine learning pipeline.\n",
    "\n",
    "Features linked to the customer:\n",
    "\n",
    "* Insured NIF: Customer ID \n",
    "* Age at signature: Customers age at insurance contract signature\n",
    "* Sex\n",
    "* Policy Holder date of birth: customer date of birth\n",
    "\n",
    "Features linked to insurance product:\n",
    "\n",
    "* Risk code: coverage to be activated for specific claim\n",
    "* Initial coverage date: insurance coverage starting date \n",
    "\n",
    "Features linked to claims:\n",
    "\n",
    "* Claim number: id of the claim\n",
    "* Claim Incident date: occurrence date of the incident\n",
    "* Declaration date: date when a customer declare its claim\n",
    "\n",
    "Features linked to the amount to be paid in case of claims:\n",
    "* Insured Amount variable for classical credit modality\n",
    "* Initial_Instalment_Amount: monthly amount paid by customer to reimburse its credit\n",
    "    \n",
    "Target to be predicted:\n",
    "\n",
    "* Refusal_Flag: Yes / No\n",
    "\n",
    "Additional information once a decision was made and you should not include in your pipeline:\n",
    "\n",
    "* Claims cause 1\n",
    "* Refused decision reason code\n",
    "* Claim_Status_Level_0\n",
    "* Refusal_Category\n",
    "\n",
    "## 3. Datacamp organization\n",
    "\n",
    "* Morning: data exploration using pandas\n",
    "* Afternoon: development of a predictive model\n",
    "\n",
    "## 4. Guideline to develop some predictive model\n",
    "\n",
    "* Use a `DummyClassifier` to get a dummy baseline.\n",
    "* Evaluate this model using cross-validation. You can use a `StratifiedKFold` strategy.\n",
    "* Check the accuracy score on both the training and testing sets. Is it an appropriate metric? Compare it with the ROC AUC.\n",
    "* Develop a predictive model using a linear model. Use the appropriate preprocessing for this model.\n",
    "* Evaluate this model with the dummy baseline.\n",
    "* Optimized the hyperparameter of your model using a `RandomizedSearchCV` or a `GridSearchCV`. Do you improve the results?\n",
    "* Can you think of additional preprocessing methods to be added to your linear model: `SplineTransformer`, `PolynomialFeatures`, etc.\n",
    "* Evaluate such a model.\n",
    "* Develop a predictive model based on `HistGradientBoostingClassifier`. Choose an appropriate preprocessing.\n",
    "* Evaluate this model.\n",
    "* Fine tune and evaluate the model again.\n",
    "* Split the dataset into a training and testing set. Compute the feature importance of this model using the function `permutation_importance`. Check the importance on both the training and testing set.\n",
    "* Instead of computing the ROC AUC, let's derive a business metric by replacing with cost and gain the entries of the confusion matrix (i.e. TP, TN, FP, FN). By default, the scikit-learn classifier will use a cut-off point at a 0.5 probability. Vary this cut-off point and compute the business metric for the different thresholds. Which cut-off point do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
