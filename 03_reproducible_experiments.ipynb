{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6701113",
   "metadata": {},
   "source": [
    "# Notebook to \"try\" reproducing certification\n",
    "\n",
    "This notebook intends to provide pieces to illustrate how to reproduce results from the \"Mémoire de Certfication\". However there are some limitations.\n",
    "\n",
    "Some of the data used in the thesis are not available in the dataset and does not allow to have the exact dataset used during the certification. Therefore the results will be different.\n",
    "\n",
    "Therefore, we will try to have a similar analysis but we will not be able to end-up with similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be99d3",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d5a04",
   "metadata": {},
   "source": [
    "We will be reusing the data that we clean during the morning of the datacamp.\n",
    "\n",
    "Note that from the original dataset, we created three new columns:\n",
    "\n",
    "- the age of the policy at the time of claim\n",
    "- the delay of the claim\n",
    "- the age of the client at the time of claim\n",
    "\n",
    "Those columns were stored as `timedelta` (cf. [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html)) expressed in days. We reuse this format and express these quantity in days to have a numerical representation that does not require complicated processing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659dd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"cleaner_dataframe.csv\",\n",
    "    parse_dates=[\n",
    "        \"Claim Incident date\",\n",
    "        \"FE_Declaration_date\",\n",
    "        \"Initial coverage date\",\n",
    "        \"First claim decision date\",\n",
    "        \"Last claim decisión date\",\n",
    "        \"Policy Holder date of birth\",\n",
    "        \"Age policy at claim\",\n",
    "    ],\n",
    "    index_col=0,\n",
    ")\n",
    "df[\"Age policy at claim\"] = pd.to_timedelta(df[\"Age policy at claim\"]).dt.days\n",
    "df[\"Delay declaration\"] = pd.to_timedelta(df[\"Delay declaration\"]).dt.days\n",
    "df[\"Age client at claim\"] = pd.to_timedelta(df[\"Age client at claim\"]).dt.days\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ee325",
   "metadata": {},
   "source": [
    "We can check that opening the CSV file when as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e839bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65714a34",
   "metadata": {},
   "source": [
    "As we can see, dates have been parsed properly and our three new columns are stored with integral data types.\n",
    "\n",
    "Now, we will split the dataset into a vector containing whether or not a claim was refused and a matrix containing the features. For this matter, we will store the name of the features and the name of the target.\n",
    "\n",
    "Note that we exclude the date from the features used because we created the last three columns using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "#     \"Risk code\",\n",
    "    \"Insured amount\",\n",
    "    \"Initial_Instalment_Amount\",\n",
    "    \"Age at signature\",\n",
    "    \"Sexo\",\n",
    "    \"Age policy at claim\",\n",
    "    \"Delay declaration\",\n",
    "    \"Age client at claim\",\n",
    "]\n",
    "target_names = \"Refusal_Flag\"\n",
    "X, y = df[feature_names], df[target_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e27523",
   "metadata": {},
   "source": [
    "Thus, we have 7 features to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8dd29",
   "metadata": {},
   "source": [
    "### Aside note regarding categorical features\n",
    "\n",
    "As we saw during the lecture, categorical features need particular attention: these features are necessarly numerical values while machine learning algorithm only deal with such numerical values. We therefore needs to encode those values to a numerical representation.\n",
    "\n",
    "However, this representation needs to be chose with care. Indeed, it will depend on the type of predictive model used later on.\n",
    "\n",
    "Here, we recall the two main type of categorical encoders used. We also recall with which model one wants to use them.\n",
    "\n",
    "First let's define in a Python list, which columns can be considered categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a33ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "#     \"Risk code\",\n",
    "    \"Sexo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7bc92c",
   "metadata": {},
   "source": [
    "Indeed, we have two categorical features. While it is obvious that `\"Sexo\"` is such a categorical features, it is a less obvious for `\"Risk code\"` since it is represented already by some integral values. However, the name indicate us that this is a \"code\" and therefore composed of discrete values.\n",
    "\n",
    "We should therefore include this column in our processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c012f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical = X[categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a75dc",
   "metadata": {},
   "source": [
    "#### Ordinal encoding\n",
    "\n",
    "The simplest encoding that we can use is to replace each category by an integer. We will therefore have categories represented by values between `[0, n_categories - 1]`. We will illustrate below such encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_categorical)\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573b50e",
   "metadata": {},
   "source": [
    "The output of scikit-learn will be a NumPy array. We can create a pandas `DataFrame` using the `get_feature_names_out` method to generate the column names. This method allows to track complex column transformation in scikit-learn `Pipeline` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = pd.DataFrame(\n",
    "    X_train_encoded, columns=encoder.get_feature_names_out()\n",
    ")\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9b4ef",
   "metadata": {},
   "source": [
    "We observe that each categories is therefore encoded with an integer. We could check the correspondence by looking at the fitted attributes of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f359956",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name, col_categories in zip(\n",
    "    encoder.get_feature_names_out(),\n",
    "    encoder.categories_\n",
    "):\n",
    "    print(f\"For feature named {col_name!r}\")\n",
    "    print()\n",
    "    for code, cat in enumerate(col_categories):\n",
    "        print(f\"\\tCategory {cat!r} mapped to {code}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171b9da",
   "metadata": {},
   "source": [
    "This type of encoder is imposing an ordering and it is not always adequate for linear model in which we don't want to impose such modeling. It is therefore the encoder of choice used in tree-based models.\n",
    "\n",
    "#### One-hot encoding\n",
    "\n",
    "When using a linear model, we will be \"usually\" interested of having an independence between categories. Therefore, with such models, `OneHotEncoder` is an interesting encoder. We will reproduce the previous experiment to show what this encoder does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ed015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_categorical)\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = pd.DataFrame(\n",
    "    X_train_encoded.A, columns=encoder.get_feature_names_out()\n",
    ")\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae90263",
   "metadata": {},
   "source": [
    "We see that each category becomes a column and that an indicator 0/1 is used to indicate whether or not we deal with the category for this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55b0df",
   "metadata": {},
   "source": [
    "### Creating basic predictive models\n",
    "\n",
    "Now, we can design some predictive model. Here, we will develop 3 predictive models:\n",
    "\n",
    "- a dummy classifier that is not going to use `X` to make any prediction. It is not an interesting model but it provides a kind of baseline.\n",
    "- a linear model with an adequate preprocessing. Indeed, the preprocessing has to contain a scaler and a one-hot encoder.\n",
    "- a gradient boosting decision trees model. This model should use an ordinal encoder in its preproceesing stage.\n",
    "\n",
    "Let's define the three models.\n",
    "\n",
    "#### Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_model = DummyClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb173907",
   "metadata": {},
   "source": [
    "#### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecfdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the numerical columns (the ones that are not categorical)\n",
    "numerical_columns = X.columns.difference(categorical_columns).tolist()\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"categorical_processor\", OneHotEncoder(drop=\"if_binary\"), categorical_columns),\n",
    "    (\"numerical_processor\", StandardScaler(), numerical_columns),\n",
    "])\n",
    "linear_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())\n",
    "])\n",
    "linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f9e2b",
   "metadata": {},
   "source": [
    "#### Tree-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cce582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        (\"categorical_processor\", OrdinalEncoder(), categorical_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "hgbdt_model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", HistGradientBoostingClassifier(random_state=0)),\n",
    "])\n",
    "hgbdt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682ffe9",
   "metadata": {},
   "source": [
    "### Evaluating these models with cross-validation\n",
    "\n",
    "Now that we define these models, we have to evaluate them. For this purpose, we need to use a metric. However, our problem is imbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572cfa8",
   "metadata": {},
   "source": [
    "Indeed, the proportion of `\"No\"` and `\"Yes\"` is not 50-50. We therefore need to select a proper statistical metric. Here, we can use the ROC-AUC.\n",
    "\n",
    "We also need to choose a cross-validation strategy. A stratified k-fold cross-validation is a good choice in this case to be sure that the ratio of the classes is preserved in each of the splits. In addition, we will shuffle the data. I will come back on this point.\n",
    "\n",
    "Let's evaluate the dummy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=30, shuffle=True)\n",
    "cv_results = cross_validate(\n",
    "    dummy_model, X, y,\n",
    "    return_train_score=True,\n",
    "    scoring=[\"roc_auc\"],\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results = cv_results[[\"train_roc_auc\", \"test_roc_auc\"]]\n",
    "print(\n",
    "    f\"\"\"Dummy classifier results in terms of ROC-AUC:\n",
    "\n",
    "    Training: {cv_results['train_roc_auc'].mean():.3f} +/- {cv_results['train_roc_auc'].std():.3f}\n",
    "    Testing: {cv_results['test_roc_auc'].mean():.3f} +/- {cv_results['test_roc_auc'].std():.3f}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6079d",
   "metadata": {},
   "source": [
    "It is not a surprised that this model outputs an average AUC of 0.5 on both training and testing. Let's check if a linear model provides better statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883fadc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    linear_model, X, y,\n",
    "    return_train_score=True,\n",
    "    scoring=[\"roc_auc\"],\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results = cv_results[[\"train_roc_auc\", \"test_roc_auc\"]]\n",
    "print(\n",
    "    f\"\"\"Linear model results in terms of ROC-AUC:\n",
    "\n",
    "    Training: {cv_results['train_roc_auc'].mean():.3f} +/- {cv_results['train_roc_auc'].std():.3f}\n",
    "    Testing: {cv_results['test_roc_auc'].mean():.3f} +/- {cv_results['test_roc_auc'].std():.3f}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d328fd",
   "metadata": {},
   "source": [
    "It seems that this is a bit better. We can also see that the model does not overfit since the results are similar on the training and testing set. Let's check the last model type of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4fcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    hgbdt_model, X, y,\n",
    "    return_train_score=True,\n",
    "    scoring=[\"roc_auc\"],\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "print(\n",
    "    f\"\"\"HGBDT model results in terms of ROC-AUC:\n",
    "\n",
    "    Training: {cv_results['train_roc_auc'].mean():.3f} +/- {cv_results['train_roc_auc'].std():.3f}\n",
    "    Testing: {cv_results['test_roc_auc'].mean():.3f} +/- {cv_results['test_roc_auc'].std():.3f}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107441d",
   "metadata": {},
   "source": [
    "We observe that this model is better than the previous one. we can see that there is a small overfitting since the model is performing worse on the testing set than on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384e975",
   "metadata": {},
   "source": [
    "### Reproducing ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06815f8c",
   "metadata": {},
   "source": [
    "In the thesis, the structure was broken to report the results because a single train-test split was used with shuffling. We can reproduce the ROC curve using such approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model.fit(X_train, y_train)\n",
    "linear_model.fit(X_train, y_train)\n",
    "_ = hgbdt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de60d2",
   "metadata": {},
   "source": [
    "Now we can use the plotting tool from scikit-learn to plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    dummy_model, X_test, y_test, name=\"Dummy classifier\", linestyle=\"--\"\n",
    ")\n",
    "RocCurveDisplay.from_estimator(\n",
    "    linear_model, X_test, y_test, name=\"Logistic regression\", ax=disp.ax_\n",
    ")\n",
    "RocCurveDisplay.from_estimator(\n",
    "    hgbdt_model, X_test, y_test, name=\"HGBDT classifier\", ax=disp.ax_\n",
    ")\n",
    "_ = disp.ax_.set_title(\n",
    "    \"Comparison of the ROC curve of the different models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08026abf",
   "metadata": {},
   "source": [
    "We more or less obtain the results show in the tesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85516b0",
   "metadata": {},
   "source": [
    "### Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62775820",
   "metadata": {},
   "source": [
    "Here, we show how to plot the feature importances by using the permutation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce77e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "importances = permutation_importance(\n",
    "    hgbdt_model, X_test, y_test, scoring=\"roc_auc\"\n",
    ")\n",
    "importances = pd.DataFrame(\n",
    "    importances.importances.T,\n",
    "    columns=hgbdt_model[:-1].get_feature_names_out(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fb457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "_ = ax.set_xlabel(\"Decrease in terms of ROC AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab332b3",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The thesis is showing the \"Economic loss\". We cannot construct this loss because we are missing some information. However, it comes back to assign a cost to a prediction depending if we do a mistake or not.\n",
    "\n",
    "In the thesis, the cost depend on some external values that we don't have access. If we would have these access, we could there fore compute the cost, for all the possible threshold of used for the ROC curve and have a business oriented metric which is more maningful than the ROC curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
